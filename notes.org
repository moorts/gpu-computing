* GPU Computing - Exam Preparation

** Basics
*** Types of Parallelism
- Instruction-Level Parallelism
  - One instruction stream
  - A lot of dependencies and branches
  - Very Limited
- Thread Level Parallelism
  - Multiple independent instruction streams
  - No branching for parallelism
  - Limited by max. executable I-streams
- Data Level Parallelism
  - One Operation - Multiple independent elements
  - Parallelism dependent on data structure
  - e.g. Vectorization
- Request Level Parallelism
  - Datacenters
  - Many requests from many users
***  Data Access
- Data Locality
  - Let threads operate on data that is close in memory
  - Leads to more efficient caching -> higher bandwidth

- Non-Uniform Memory Access (NUMA)
  - Memory divided into seperate regions
  - Threads are assigned to one region each
  - Accesses inside the proper region are very fast

*** Communication
- Plain load/store (LD/ST) - shared mem systems
  - Threads just store and load from shared mem directly
  - No communication
  - Good for SMP, bad for NUMA
- Message passing (SP)
  - send(X, 1, tag), recv(Y, 0, tag) - Communication through send/rec functions
  - p2p and collective functions
  - overhead: copying, matching, progress, ordering
- Other models
  - Active messages - control latency on software level
  - One-sided communication (put/get) - never say receive
*** Algorithm Design
**** Foster's PCAM
- Partitioning
  - Ignore technical task
  - Divide problem into subproblems
    - Domain Decomposition (divide the data, e.g. for structures like trees or arrays)
    - Functional Decomposition (divide different kinds of tasks, e.g. different climate computing models (atmosphere, water, etc.)
    - Pipeline Decomposition (Divide into tasks that are dependent of each other, e.g. CPU instruction pipelining)
- Communication
  - Concurrent execution of partitions, not completely independent
  - Data dependencies -> communication
    - Local/Global (communication within partition vs between partitions)
    - Structured/Unstructured (structure of communicated data is static vs changes over time)
    - Static/Dynamic (Always the same communication vs ...not?)
    - Synchronous vs Asynchronous (wait for requested data (sleep) vs carry on execution after requesting data)
- Agglomeration
  - Increase Granularity
    - Abstract to Concrete (?)
    - Number of tasks T >= number of processors P (parallel slackness)
  - Reduce Communication Costs
    - e.g. data replication
  - Depending on use case:
    - BSP: order of magnitude more Ts than Ps
    - HPC: T = P
    - SIMD: T = 1
- Mapping
  - Assignment: task <-> processor & memory
    - Distribute tasks over processors
    - Execute independent task concurrently
    - Task Locality (place tasks that communicate often on the same processor)
  - Not required for:
    - Uni-processors or shared mem systems with auto mapping
    - When scheduling is implemented by the OS or the hardware
  - Mapping is NP-Complete
***  BSP Model
- Parallel Slackness
  - Programs written for v virtual parallel processors
  - run on p physical processors
  - v >> p (e.g. v = p log p ???)
**** The Model
The BSP Model is defined as the combination of three attributes:
- Components (processing and/or memory functions)
- A Router (message delivery between paris of components)
- Synchronization Facilities
  - Sync at regular intervals of L time units (L is called periodicity parameter)
  - Computation is divied into supersteps
- Superstep:
  - Allocate task for each component
    - Task = combination of computation steps and communication
  - After L timesteps: Check if each component finished its task
    - Finished: Move on to next superstep
    - Unfinished: Allocate another L time units to finish the step
**** What Does it Mean?
** Algorithms
*** Matrix Multiplication
- GPU Naive Algorithm
#+begin_src C
  __global__ void matrixMultNaive(float* Md, float* Nd, float* Pd, int Width)
  {
      float PValue = 0; // intermediate result
      float Melement, Nelement;

      for(int k = 0; k < Width; ++k) {
          Melement = Md[threadIdx.y * Width + k];
          Nelement = Nd[k * Width + threadIdx.x];
          Pvalue += Melement * Nelement;
      }
      Pd[threadIdx.y * Width + threadIdx.x] = Pvalue;
  }
#+end_src
*Analysis*
Per loop this performs 2 FLOPS and 4 memory accesses.
-> memory = 4N^3, computational load = 2*N^3
-> I = 1/2 (computational intensity)
-> FLOPS/Byte = 1/8
Single block computes entire Matrix
-> Matrix size limited by threads/block
- Using Multiple Thread Blocks
#+begin_src C
  __global__ void matrixMultTiled(float* Md, float* Nd, float* Pd, int Width)
  {
      float PValue = 0; // intermediate result
      float Melement, Nelement;

      int row = blockIdx.y * blockDim.y + threadIdx.y;
      int col = blockIdx.x * blockDim.x + threadIdx.x;

      for(int k = 0; k < Width; ++k) {
          Melement = Md[row * Width + k];
          Nelement = Nd[k * Width + col];
          Pvalue += Melement * Nelement;
      }
      Pd[row * Width + col] = Pvalue;
  }
#+end_src
*Analysis*
Each thread uses global memory, 2 32bit accesses per SP Multiply-Add -> 4B per FLOP
=> 13 TFLOPs need 52 TB/s memory bandwidth (RTX 2080Ti has 616 GB/s)
=> Performance limited to ~150 GFLOPs/sec
- Shared Memory Optimizations
  - TxT tile uses each element T times
  - Calculate only parts of the elements of C to group close elements together
  - Each Tile loads one element from the original matrix into shared memory
  - Then each thread does one partial vector product:
    - rows[ty][tileWidth*tx:tileWidth*(tx+1)] * cols[tx][tileWidth*ty:tileWidth*(ty+1)]
  - Two synchronization points required, one after loading into shared, one after the computation
#+begin_src C
  __global__ void matrixMultShared(float* Md, float* Nd, float* Pd, int Width)
  {
      // SIZE = 2 * TILEWIDTH*TILEWIDTH
      __shared__ float sh_mem[];

      float* Mds = sh_mem;
      float* Nds = (float*) &sh_mem[TILEWIDTH*TILEWIDTH];

      float PValue = 0; // intermediate result
      float Melement, Nelement;

      int tx = threadIdx.x;
      int ty = threadIdx.y;
      int bx = blockIdx.x;
      int by = blockIdx.y;

      int row = by * TILEWIDTH + ty;
      int col = bx * TILEWIDTH + tx;

      for(int m = 0; m < Width / TILEWIDTH; ++m) {
          Mds[ty][tx] = Md[row * Width + (m * TILEWIDTH + tx)]
          Nds[ty][tx] = Nd[(m * TILEWIDTH + ty) * WIDTH + col]
          __syncthreads();

          for(int k = 0; k < TILEWIDTH; ++k) {
              Pvalue += Mds[ty][k] * Nds[k][tx]
          }
          __syncthreads();
      }
      Pd[row * Width + col] = Pvalue;
  }
#+end_src
- Further Optimizations
  - Multiple Output values per Thread
  - Bank conflicts (possibly detectable using nvprof)
  - Vectorization using float2/float4
  - Double buffering
    - Loading next set of shared memory into one buffer while data from other buffer is still in use
    - Twice the required shared memory
    - Hides Latency very effectively

** Performance and Evaluation

*** Roofline Model
*Model Parameters*
- Bandwidth of the System b_s (Bytes/sec) 
  - Theoretical limit of the hardware
  - Machine parameter
- Computational Intensity I (Flops/Byte)
  - How much work is done per byte of data.
  - Code characteristic

- Attainable Flops/sec
  - Execution of work p_peak (Flops/sec)
    - Machine parameter
  - Data Path I * b_s (Flops/Byte x Bytes/sec)

*Roofline*
- Measure I against P = min(p_peak, I*b_s)
  - Creates "Roofline"-Shape
  - I*b_s is linearly increasing over I
  - p_peak is constant over I -> horizontal line

*Assumptions + Consequences*
- Optimistic Model ("light speed")
  - Always gives upper bound
- Data transfer and core execution overlap perfectly
- Latency effects are ignored
- Knee Point: Intersection of p_peak with I*b_s
  - I before knee-point: Memory-bound program
    - Harder to optimize:
    - Loop Restructuring to improve memory access patterns
    - Memory Affinity (stuff like NUMA regions.... I think?)
    - Software Prefetching
  - I after knee-point: Compute-bound
    - Easier to Optimize
    - Loop Unrolling
    - SIMD
      
